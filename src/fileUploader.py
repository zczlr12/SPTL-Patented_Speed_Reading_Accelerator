import os
import re
import json
import datetime
import math
import fitz
import pandas as pd
import streamlit as st
import socket
import logging
import base64
import requests
import openai
import tiktoken
from pyecharts.charts import Tree
from pyecharts import options as opts
from streamlit_echarts import st_pyecharts
from PIL import Image
import platform
import pytesseract
from mainPage import config, auxiliary, limit
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
    retry_if_exception_type
)  # for exponential backoff

hostname = socket.gethostname()
ip = socket.gethostbyname(hostname)
logging.basicConfig(filename='record.log', format=f'%(asctime)s - {ip} - %(levelname)s: %(message)s',
                    level=logging.INFO)

openai.api_key = "fa836280571c4c5ba3130e9ff80b44fd"    # Azure çš„å¯†é’¥
openai.api_base = "https://myopenairesource-test001.openai.azure.com/"  # Azure çš„ç»ˆç»“ç‚¹
openai.api_type = "azure"
openai.api_version = "2023-06-01-preview"  # API ç‰ˆæœ¬ï¼Œæœªæ¥å¯èƒ½ä¼šå˜
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

if platform.system() == 'Windows':
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

subtitles = ('æ‘˜è¦', 'æƒåˆ©è¦æ±‚', 'æ‰€å±é¢†åŸŸ', 'èƒŒæ™¯æŠ€æœ¯', 'ç›®çš„', 'æŠ€æœ¯æ–¹æ¡ˆ', 'æœ‰ç›ŠæŠ€æœ¯æ•ˆæœ')
ch_en = (('æ‘˜è¦', 'ABSTRACT'), ('æ‰€å±é¢†åŸŸ', 'FIELD'), ('èƒŒæ™¯æŠ€æœ¯', 'BACKGROUND'), ('æŠ€æœ¯æ–¹æ¡ˆ', 'SUMMARY'))


# åœ¨æ ‘ä¸­æ·»åŠ æ–‡æœ¬
def add_item(children_list, name):
    children_list.append({"name": name, "children": [{}]})


def split_paragraph(content):
    paragraphs = re.split(r'[\[ã€]\d{4}[]ã€‘ï¼½]', content)  # ä»¥â€[00xx]â€åˆ†æ®µ
    if len(paragraphs) <= 1:
        paragraphs = re.split(r'\s+\n|\n\s+', content)
    if len(paragraphs) <= 1:
        paragraphs = content.split('\n')
    return [re.sub(r'\s*\n\s*', '', paragraph) for paragraph in paragraphs if len(paragraph) > 3]


# ä»¥substringä½œä¸ºæ®µè½å¼€å¤´åˆ†æ®µ
def split_claims(substring, content):
    indexes = []  # æ®µè½é¦–ä¸ªå­—ç¬¦åœ¨contentä¸­çš„ç´¢å¼•å€¼
    res = []
    f = re.finditer(substring, content, re.DOTALL)
    for i in f:
        indexes.append(i.span()[0])
    if not indexes:
        return content
    for i in range(len(indexes)):
        if indexes[i] != indexes[-1]:
            res.append(content[indexes[i]:indexes[i+1]])
        else:
            res.append(content[indexes[-1]:])
    return res


# è‡ªåŠ¨æ¢è¡Œ
def line_wrap(text, width=29):
    wrapped_text = ""
    char_num = len(text)
    row_num = math.ceil(char_num / width)
    if char_num <= width:
        return text
    for i in range(row_num):
        start = i * width
        end = start + width
        if i != row_num - 1:
            wrapped_text += text[start:end] + '\n'
        else:
            wrapped_text += text[start:]
    return wrapped_text


def search_content(pattern, content):
    try:
        content = re.search(pattern, content, re.DOTALL).group(1)
        return re.sub(r'[\[ã€]\d{4}[]ï¼½ã€‘]', '\n', content)
    except (TypeError, AttributeError):
        return ""


# ç¿»è¯‘å‡½æ•°ï¼Œword éœ€è¦ç¿»è¯‘çš„å†…å®¹
def translate(text):
    # æœ‰é“è¯å…¸ api
    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=null'
    # ä¼ è¾“çš„å‚æ•°ï¼Œå…¶ä¸­ i ä¸ºéœ€è¦ç¿»è¯‘çš„å†…å®¹
    key = {
        'type': "AUTO",
        'i': text,
        "doctype": "json",
        "version": "2.1",
        "keyfrom": "fanyi.web",
        "ue": "UTF-8",
        "action": "FY_BY_CLICKBUTTON",
        "typoResult": "true"
    }
    # key è¿™ä¸ªå­—å…¸ä¸ºå‘é€ç»™æœ‰é“è¯å…¸æœåŠ¡å™¨çš„å†…å®¹
    response = requests.post(url, data=key)
    # åˆ¤æ–­æœåŠ¡å™¨æ˜¯å¦ç›¸åº”æˆåŠŸ
    if response.status_code == 200:
        out = ''
        in_outs = json.loads(response.text)['translateResult'][0]
        for in_out in in_outs:
            if in_out['tgt']:
                out += in_out['tgt']
            else:
                return "ç¿»è¯‘ç»“æœä¸ºç©º"
        return out
    return "æœ‰é“è¯å…¸è°ƒç”¨å¤±è´¥"


# æŠŠæ–‡æœ¬æ”¹æˆè¯¥å†™å…¥å¶å­èŠ‚ç‚¹çš„æ ¼å¼
@retry(  # é˜²æ­¢å› è¿æ¥é—®é¢˜æŠ¥é”™
    retry=retry_if_exception_type((openai.error.APIConnectionError, openai.error.ServiceUnavailableError,
                                   openai.error.Timeout, openai.error.APIError)),
    wait=wait_random_exponential(multiplier=1, max=60),
    stop=stop_after_attempt(10)
)
def paragraphing(num, content, if_translate, summarize=False, lang='ä¸­å›½', df=None, title=None, index=""):
    prompt = "å°†ä»¥ä¸‹æ®µè½æ¦‚æ‹¬åˆ°50å­—ä»¥å†…ï¼š"
    if lang != 'ä¸­å›½':
        prompt = "Summarize the following paragraph(s) in 50 words:"
    content = re.sub(r"\s*\n\s*", "", content)  # å»é™¤æ¢è¡Œç¬¦
    if content:
        if summarize:
            with open('noOfAccesses.json') as f:
                access_num = json.load(f)
            if access_num[1] < 100:
                today = datetime.date.today().strftime('%Y-%m-%d')
                original = content
                completion = openai.ChatCompletion.create(  # gpt-35-turbo
                    engine="Test001ToCheckIfSuccess",
                    messages=[{"role": "system", "content": prompt + '\n' + content}]
                )
                content = completion['choices'][0]["message"]["content"]
                df.loc[title + index] = [len(encoding.encode(original)), len(encoding.encode(content))]
                if access_num[0] != today:
                    access_num = [today, 0]
                access_num[1] += 1
                num.progress(access_num[1], auxiliary(access_num[1]))
                with open('noOfAccesses.json', 'w') as f:
                    json.dump(access_num, f)
            else:
                content = "è°ƒç”¨æ¬¡æ•°å·²è¾¾åˆ°ä¸Šé™"
        if if_translate and not re.search(r'[\u4e00-\u9fa5]', content):
            content = translate(content)
    else:
        content = "æš‚æœªè·å–"
    if index:
        content = index + "ã€" + content
    return line_wrap(content), df  # è¿”å›è‡ªåŠ¨æ¢è¡Œä¹‹åçš„å†…å®¹


def display_label(path, content):
    if os.path.exists(path):
        return "ğŸ”é‡æ–°" + content
    return "ç¡®è®¤" + content


def ocr_reader(page, page_index, option):
    ocr_ports = {'é€šé“äºŒ': (8000, 'PaddleOCR'), 'é€šé“ä¸‰': (8010, 'EasyOCR')}
    rotate = int(0)
    # æ¯ä¸ªå°ºå¯¸çš„ç¼©æ”¾ç³»æ•°ä¸º1.3ï¼Œè¿™å°†ä¸ºæˆ‘ä»¬ç”Ÿæˆåˆ†è¾¨ç‡æé«˜2.6çš„å›¾åƒã€‚
    # æ­¤å¤„è‹¥æ˜¯ä¸åšè®¾ç½®ï¼Œé»˜è®¤å›¾ç‰‡å¤§å°ä¸ºï¼š792X612, dpi=96
    zoom_x = 4  # (æ–°å‹é¢…å†…æ”¯æ¶Enterprise_çœç•¥_å¼¹ç°§åœˆæ “å¡æ²»ç–—é¢…å†…å¾®å°å®½é¢ˆåŠ¨è„‰ç˜¤_é»„æµ·ä¸œ.33333333-->1056x816)   (2-->1584x1224)
    zoom_y = 4
    mat = fitz.Matrix(zoom_x, zoom_y).prerotate(rotate)
    pix = page.get_pixmap(matrix=mat, alpha=False)
    pix._writeIMG("page.png", 1)
    with Image.open("page.png") as im:
        if page_index == 0:
            im = im.crop((100, 1000, 1260, 3200))
            im.save("page.png")
        if option == 'é€šé“ä¸€':
            return pytesseract.image_to_string(im, 'chi_sim')
    with open("page.png", 'rb') as f:
        data = f.read()
    data = base64.b64encode(data).decode('ascii')
    try:
        return json.loads(requests.post(f"http://127.0.0.1:{ocr_ports[option][0]}/{ocr_ports[option][1]}/",
                                        json={"data": data}).content)
    except Exception as e:
        st.error(e)


def identification(file_name):
    with fitz.open("./inputs/" + file_name) as doc:
        return not doc[0].get_text("text")


def preprocess(texts):
    text = '\n'.join(texts).strip()
    if re.search(r'[\[{]\d{4}]', text):
        text.replace('\n', '')
        return re.sub(r'[\[{]\d{4}]', '\n', text.replace('\n', '')).strip()
    return text


def extract_paragraphs(file_name, data_file, ocr_option):
    data = {'åç§°': '', 'æ‘˜è¦': '', 'æƒåˆ©è¦æ±‚': [], 'æ‰€å±é¢†åŸŸ': '', 'èƒŒæ™¯æŠ€æœ¯': '', 'ç›®çš„': [], 'æŠ€æœ¯æ–¹æ¡ˆ': [], 'æœ‰ç›ŠæŠ€æœ¯æ•ˆæœ': []}
    bar = None
    if ocr_option:
        bar = st.progress(0.0, f'æ­£åœ¨åˆå§‹åŒ–OCR')
    with fitz.open("./inputs/" + file_name) as doc:
        claims = ""
        instr = ""
        total = doc.page_count
        for i in range(total):
            page = doc[i]
            text = page.get_text("text", sort=True)  # æŒ‰é¡ºåºæå–æ–‡å­—
            if ocr_option:
                bar.progress(i / total, f'æ­£åœ¨è¯»å–ç¬¬{i + 1}/{total}é¡µâ€¦â€¦')
                text = ocr_reader(page, i, ocr_option)
            if i == 0:
                try:
                    data['åç§°'] = re.compile(r"åç§°(.+)\S{4}\s*æ‘˜è¦", re.DOTALL).search(text).group(1)  # æ ¹èŠ‚ç‚¹
                except (TypeError, AttributeError):
                    data['åç§°'] = re.split(r'-|_', file_name, 1)[-1]
                data['æ‘˜è¦'] = search_content(r"æ‘˜è¦(.+ã€‚)", text)
            elif re.search(r'æƒ\s*åˆ©\s+è¦\s*æ±‚\s*ä¹¦', text, re.DOTALL):  # æƒåˆ©è¦æ±‚ä¹¦
                try:
                    claims += re.search(r'é¡µ(.+[^\d\s])', text, re.DOTALL).group(1)
                except (TypeError, AttributeError):
                    claims += text
            else:  # è¯´æ˜ä¹¦
                try:
                    instr += re.search(r'é¡µ(.+[^\d\s])', text, re.DOTALL).group(1)
                except (TypeError, AttributeError):
                    instr += text
    if ocr_option:
        bar.progress(1.0, 'è¯»å–å®Œæˆ')

    purpose = ""
    method = ""
    rest = []
    methods = []
    benefits = []
    is_benefits = False
    is_purpose = False
    start = False
    # æƒåˆ©è¦æ±‚ä¹¦
    data['æƒåˆ©è¦æ±‚'] = split_claims(r'\n.{2,5}ä¸€\s*ç§', claims)  # æ¯æ®µå¼€å¤´ä¸º"1. ä¸€ç§"
    data['æ‰€å±é¢†åŸŸ'] = search_content(r'æŠ€æœ¯é¢†åŸŸ(.+)èƒŒæ™¯æŠ€æœ¯', instr)
    background = search_content(r'èƒŒæ™¯æŠ€æœ¯(.+)å‘æ˜å†…å®¹', instr)
    data['èƒŒæ™¯æŠ€æœ¯'] = background
    # ä¸“åˆ©å†…å®¹
    try:
        if "é™„å›¾è¯´æ˜" in instr:
            content = re.compile(r'å‘æ˜å†…å®¹(.+)é™„å›¾è¯´æ˜', re.DOTALL).search(instr).group(1)
        else:
            content = re.compile(r'å‘æ˜å†…å®¹(.+)å…·ä½“å®æ–½æ–¹å¼', re.DOTALL).search(instr).group(1)
    except (TypeError, AttributeError):
        content = ""
    paragraphs = split_paragraph(content)
    para_num = len(paragraphs)
    for i in range(para_num):
        if i == 0 and len(paragraphs[i]) <= 200:  # ä¸“åˆ©å†…å®¹ç¬¬ä¸€æ®µ
            purpose += paragraphs[i]
            if re.search(r'è¦è§£å†³çš„é—®é¢˜.?$', re.sub(r'\s', '', paragraphs[i])):
                is_purpose = True
        elif i == para_num - 1:  # ä¸“åˆ©å†…å®¹æœ€åä¸€æ®µ
            if "ä¸‹é¢" not in paragraphs[i]:
                benefits.append(paragraphs[-1])
        else:
            if re.search(r'è§£å†³é—®é¢˜çš„æ–¹æ³•.?$', re.sub(r'\s', '', paragraphs[i])):
                is_purpose = False
            if is_purpose:
                purpose += paragraphs[i]
            elif is_benefits:
                benefits.append(paragraphs[i])
            elif re.search(r'æœ‰ç›Šæ•ˆæœ|ä¼˜ç‚¹', re.sub(r'\s', '', paragraphs[i])):  # æœ‰ç›ŠæŠ€æœ¯æ•ˆæœèµ·å§‹æ®µ
                is_benefits = True
                if len(paragraphs[i]) > 30:
                    benefits.append(paragraphs[i])
            else:
                rest.append(paragraphs[i])
    for paragraph in rest:  # æŠ€æœ¯æ–¹æ¡ˆå¯ä»¥åˆ†æ®µçš„æƒ…å†µ
        if re.match(r"[^\u4e00-\u9fa5]*ä¸€ç§", paragraph) or re.search(r"æä¾›.*ä¸€ç§", paragraph) or \
                re.search(r"æå‡º.*ä¸€ç§", paragraph):
            start = True
            if method:
                methods.append(method)
            method = ""
        if start:
            method += paragraph
    methods.append(method)
    if not methods[0]:  # æŠ€æœ¯æ–¹æ¡ˆæ— æ³•åˆ†æ®µçš„æƒ…å†µ
        methods = []
        for paragraph in rest:
            method += paragraph
        methods.append(method)
    if not purpose:
        try:
            purpose = split_paragraph(background)[-1]
        except IndexError:
            pass
    temp = re.split(r'ï¼Œ|,', purpose, 1)
    if len(temp[0]) < 8:
        purpose = temp[-1]
    if len(benefits) > 4:
        benefits = ''.join(benefits)
    data['ç›®çš„'] = purpose
    data['æŠ€æœ¯æ–¹æ¡ˆ'] = methods
    data['æœ‰ç›ŠæŠ€æœ¯æ•ˆæœ'] = benefits
    with open(data_file, 'w') as f:
        json.dump(data, f)


def extract_paragraphs_en(file_name, data_file):
    result = ""
    results = []
    subtitles_en = []
    data = {'åç§°': re.split(r'-|_', file_name, 1)[-1].replace('.pdf', '').replace('+', ' '), 'æ‘˜è¦': '', 'æƒåˆ©è¦æ±‚': '',
            'æ‰€å±é¢†åŸŸ': '', 'èƒŒæ™¯æŠ€æœ¯': '', 'ç›®çš„': '', 'æŠ€æœ¯æ–¹æ¡ˆ': '', 'æœ‰ç›ŠæŠ€æœ¯æ•ˆæœ': ''}
    bar = st.progress(0.0, f'æ­£åœ¨è¯»å–â€¦â€¦')
    with fitz.open("./inputs/" + file_name) as doc:
        total = doc.page_count
        for i in range(total):
            page = doc[i]
            bar.progress(i / total, f'æ­£åœ¨è¯»å–ç¬¬{i + 1}/{total}é¡µâ€¦â€¦')
            rotate = int(0)
            # æ¯ä¸ªå°ºå¯¸çš„ç¼©æ”¾ç³»æ•°ä¸º1.3ï¼Œè¿™å°†ä¸ºæˆ‘ä»¬ç”Ÿæˆåˆ†è¾¨ç‡æé«˜2.6çš„å›¾åƒã€‚
            # æ­¤å¤„è‹¥æ˜¯ä¸åšè®¾ç½®ï¼Œé»˜è®¤å›¾ç‰‡å¤§å°ä¸ºï¼š792X612, dpi=96
            zoom_x = 4  # (æ–°å‹é¢…å†…æ”¯æ¶Enterprise_çœç•¥_å¼¹ç°§åœˆæ “å¡æ²»ç–—é¢…å†…å¾®å°å®½é¢ˆåŠ¨è„‰ç˜¤_é»„æµ·ä¸œ.33333333-->1056x816)   (2-->1584x1224)
            zoom_y = 4
            mat = fitz.Matrix(zoom_x, zoom_y).prerotate(rotate)
            pix = page.get_pixmap(matrix=mat, alpha=False)
            pix._writeIMG("page.png", 1)
            with Image.open("page.png") as im:
                if i == 0:
                    abstract = pytesseract.image_to_string(im.crop((1240, 325, 2180, 3015)))
                    try:
                        abstract = re.search(r'ABSTRACT(.+\.)', abstract, re.DOTALL).group(1)
                        data['æ‘˜è¦'] = re.sub(r' \(\d+\)', '', abstract.replace('-\n', '').replace('\n', ' ')).strip()
                    except AttributeError:
                        pass
                else:
                    result += '\n' + pytesseract.image_to_string(im)
    bar.progress(1.0, 'è¯»å–å®Œæˆ')
    paragraphs = result.split('\n\n')
    for paragraph in paragraphs:
        if len(paragraph) > 2 and (len(paragraph) > 20 or paragraph.isupper() or re.match(r'\d', paragraph)):
            results.append(paragraph.replace('-\n', '').replace('\n', ' '))
    for i, paragraph in enumerate(results):
        if paragraph.isupper():
            subtitles_en.append((i, paragraph))
        elif re.match(r'1\.|1-\d+\.', paragraph):
            data['æƒåˆ©è¦æ±‚'] = '\n'.join(results[i:])
    for i, subtitle_en in enumerate(subtitles_en):
        for ch, en in ch_en:
            if en in subtitle_en[1]:
                data[ch] = preprocess(results[subtitle_en[0] + 1: subtitles_en[i + 1][0]])
    with open(data_file, 'w') as f:
        json.dump(data, f)


def data2tree(number, data_file, tree_file, lang, if_translate, summarize, df, statistics, num):
    tree_list = []
    summarize_progress = None
    with open(data_file) as f:
        data = json.load(f)
    if summarize:
        summarize_progress = st.progress(0.0)
    summarize_index = 0
    for subtitle in subtitles:
        children = []
        contents = data[subtitle]
        summarization = subtitle in summarize
        if summarization:
            summarize_progress.progress(summarize_index / len(summarize), f'æ­£åœ¨æ¦‚æ‹¬â€œ{summarize[summarize_index]}â€â€¦â€¦')
            summarize_index += 1
        if type(contents) == list:
            if not contents:
                contents = ['']
            content_num = len(contents)
            for i in range(content_num):
                res, df = paragraphing(num, contents[i], if_translate, summarization, lang, df, subtitle,
                                       str(i + 1) if content_num > 1 else '')
                children.append({'name': res, 'children': [{}]})
        else:
            res, df = paragraphing(num, contents, if_translate, summarization, lang, df, subtitle)
            children.append({'name': res, 'children': [{}]})
        tree_list.append({'name': subtitle, 'children': children})
    if summarize:
        summarize_progress.progress(1.0, f'æ¦‚æ‹¬å®Œæˆã€‚')
    total = df.sum(0)
    price = total.sum() * 0.00004
    with open("prices.json") as f:
        price_data = json.load(f)
    today = datetime.date.today().strftime('%Y-%m-%d')
    if today in price_data:
        price_data[today] += price
    else:
        price_data[today] = price
    with open("prices.json", 'w') as f:
        json.dump(price_data, f)
    df.loc['æ€»è®¡'] = total
    df.to_csv(statistics)
    series = [{'name': number + '\n' + data['åç§°'], 'children': tree_list}]
    with open(tree_file, 'w') as f:
        json.dump(series, f)


def uploader(num):
    st.header('æ‰¹é‡è®¾ç½®', divider="rainbow")
    col1, col2, col3, col4 = st.columns(4)
    auto = col1.checkbox('æŒ‰æ–‡ä»¶åè‡ªåŠ¨åˆ¤æ–­ä¸“åˆ©çš„å›½å®¶', True)
    if_summarize = col2.checkbox('æ‰¹é‡æ¦‚æ‹¬', True)
    if_replace = col3.checkbox('è¦†ç›–åŸæœ‰å†…å®¹', True)
    if_translate = col4.checkbox('æ¦‚æ‹¬æ—¶è‡ªåŠ¨ç¿»è¯‘å…¨è‹±æ–‡çš„å†…å®¹', True)
    left, right = st.columns(2)
    lang = left.radio('è¯·é€‰æ‹©ä¸“åˆ©çš„å›½å®¶ï¼š', ('ä¸­å›½', 'ç¾å›½'), disabled=auto, horizontal=True, captions=('', 'æš‚åªæ”¯æŒé€šé“ä¸€'))
    ocr_option = right.radio('è¯·é€‰æ‹©è¯»å–ä¸­æ–‡æ‰«æä»¶çš„OCRé€šé“ï¼š', ('é€šé“ä¸€', 'é€šé“äºŒ', 'é€šé“ä¸‰'),
                             disabled=not auto and lang != 'ä¸­å›½',
                             horizontal=True, captions=('åˆ†æ®µæ•ˆæœè¾ƒå¥½', 'é€Ÿåº¦è¾ƒå¿«', 'ç²¾åº¦è¾ƒé«˜'))
    summarize = st.multiselect('é€‰æ‹©éœ€è¦æ¦‚æ‹¬çš„æ®µè½ï¼š', subtitles, ['æƒåˆ©è¦æ±‚', 'èƒŒæ™¯æŠ€æœ¯', 'æŠ€æœ¯æ–¹æ¡ˆ'],
                               placeholder='è¯·é€‰æ‹©', disabled=not if_summarize)
    if_read = st.button("æ‰¹é‡è¯»å–ä»¥ä¸‹å·²ä¸Šä¼ çš„æ–‡ä»¶", use_container_width=True)
    st.header('ä¸Šä¼ é€šé“', divider="rainbow")
    uploaded_files = st.file_uploader('ä¸Šä¼ PDFæ–‡ä»¶', type='pdf', accept_multiple_files=True)
    if uploaded_files:
        st.session_state['files'] = uploaded_files
    for file in st.session_state['files']:
        file_name = file.name
        number = re.split(r'-|_', file_name)[0]
        data_file = f'./data/{number}.json'
        tree_file = f'./trees/{number}.json'
        statistics = f'./statistics/{number}.csv'
        df = pd.DataFrame(columns=['è¾“å…¥', 'è¾“å‡º'])
        with open("./inputs/" + file_name, "wb") as code:
            code.write(file.getvalue())
        if if_read:
            if not os.path.exists(data_file) or if_replace:
                if auto:
                    if not re.match(r'CN', number):
                        lang = 'ç¾å›½'
                    else:
                        lang = 'ä¸­å›½'
                if lang == 'ä¸­å›½':
                    temp = ocr_option
                    if not identification(file_name):
                        temp = None
                    extract_paragraphs(file_name, data_file, temp)
                else:
                    extract_paragraphs_en(file_name, data_file)
            if if_summarize and (not os.path.exists(tree_file) or if_replace):
                data2tree(number, data_file, tree_file, lang, if_translate, summarize, df, statistics, num)
        if st.button(file_name, use_container_width=True):
            st.session_state['page'] = file_name
            st.experimental_rerun()


def setup(num, file_name):
    ocr_option = None
    number = re.split(r'-|_', file_name)[0]
    data_file = f'./data/{number}.json'
    tree_file = f'./trees/{number}.json'
    mind_map = f'./mind_maps/{number}.png'
    statistics = f'./statistics/{number}.csv'
    df = pd.DataFrame(columns=['è¾“å…¥', 'è¾“å‡º'])
    st.header(number, divider="rainbow")
    with st.expander("åŸæ–‡ä»¶é¢„è§ˆ"):
        with open("inputs/" + file_name, "rb") as f:
            base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        pdf_display = f'<embed src="data:application/pdf;base64,{base64_pdf}" ' \
                      f'width="800" height="900" type="application/pdf">'
        st.markdown(pdf_display, unsafe_allow_html=True)
    left, right = st.columns(2)
    lang = left.radio('è¯·é€‰æ‹©ä¸“åˆ©çš„å›½å®¶ï¼š', ('ä¸­å›½', 'ç¾å›½'), 1 if not re.match(r'CN', number) else 0,
                      horizontal=True, captions=('', 'æš‚åªæ”¯æŒé€šé“ä¸€'))
    is_scanned = identification(file_name)
    if is_scanned:
        ocr_option = right.radio('æ£€æµ‹ä¸ºæ‰«æä»¶ï¼Œè¯·é€‰æ‹©OCRé€šé“ï¼š', ('é€šé“ä¸€', 'é€šé“äºŒ', 'é€šé“ä¸‰'), disabled=lang != 'ä¸­å›½',
                                 horizontal=True, captions=('åˆ†æ®µæ•ˆæœè¾ƒå¥½', 'é€Ÿåº¦è¾ƒå¿«', 'ç²¾åº¦è¾ƒé«˜'))
    remain = left.checkbox('è¯»å–æ—¶ä¿ç•™åŸæ€ç»´å¯¼å›¾', not is_scanned)
    if_translate = right.checkbox('è‡ªåŠ¨ç¿»è¯‘å…¨è‹±æ–‡çš„å†…å®¹', True)
    if st.button(display_label(data_file, 'è¯»å–'), use_container_width=True) \
            or not (os.path.exists(data_file) or is_scanned):
        if lang == 'ä¸­å›½':
            extract_paragraphs(file_name, data_file, ocr_option)
        else:
            extract_paragraphs_en(file_name, data_file)
        if os.path.exists(tree_file) and not remain:
            os.remove(tree_file)
        st.experimental_rerun()
    if os.path.exists(data_file):
        summarize = st.multiselect('é€‰æ‹©éœ€è¦æ¦‚æ‹¬çš„æ®µè½ï¼š', subtitles, ['æƒåˆ©è¦æ±‚', 'èƒŒæ™¯æŠ€æœ¯', 'æŠ€æœ¯æ–¹æ¡ˆ'], placeholder='è¯·é€‰æ‹©')
        if st.button(display_label(tree_file, 'æ¦‚æ‹¬'), use_container_width=True):
            data2tree(number, data_file, tree_file, lang, if_translate, summarize, df, statistics, num)
            st.experimental_rerun()
        if os.path.exists(tree_file):
            selected = []
            display = []
            with open(tree_file) as f:
                series = json.load(f)
            st.write('é€‰æ‹©éœ€è¦å±•ç¤ºçš„æ®µè½ï¼š')
            cols = st.columns(8)
            select_all = cols[0].checkbox('å…¨é€‰', True)
            for i in range(7):
                if cols[i + 1].checkbox(subtitles[i], select_all):
                    display.append(subtitles[i])
            for item in series[0]['children']:
                if item['name'] in display:
                    selected.append(item)
            series[0]['children'] = selected
            left, right = st.columns([1.4, 1])
            with left:
                height = st.number_input('è®¾ç½®é«˜åº¦', min_value=300, value=1200, step=100)
                tree = (
                    Tree(init_opts=opts.InitOpts(width="1000px", height=f"{height}px", bg_color='white'))
                    .add("",
                         series,
                         symbol_size=5,
                         pos_top="10%",
                         pos_left="10%",
                         pos_bottom="10%",
                         pos_right="10%",
                         initial_tree_depth=10,
                         label_opts=opts.LabelOpts(position="right", distance=2, font_size=10,
                                                   background_color='white'),
                         leaves_label_opts=opts.LabelOpts(position="right", distance=2, font_size=10,
                                                          background_color='white'))
                    .set_global_opts(title_opts=opts.TitleOpts(title=f"{number} æ¦‚è§ˆ"))
                )
                st_pyecharts(tree, height=f"{height}px")
                if st.button(display_label(mind_map, 'æ¸²æŸ“ä¸ºå›¾ç‰‡'), use_container_width=True):
                    with st.spinner('æ­£åœ¨æ¸²æŸ“å›¾ç‰‡â€¦â€¦'):
                        tree.render()
                        res = os.system(f"python makeSnapshot.py {number}")
                    if res:
                        st.error('å¯¼èˆªè¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚', icon='ğŸ•’')
                    else:
                        st.success('æ¸²æŸ“æˆåŠŸã€‚', icon='âœ”')
                if os.path.exists(mind_map):
                    with open(mind_map, 'rb') as png:
                        st.download_button('ä¸‹è½½å›¾ç‰‡', png, f'{number}.png', 'image/png', use_container_width=True)

            with right:
                for level2 in series[0]['children']:
                    with st.expander(level2['name']):
                        for level3 in level2['children']:
                            st.write(level3['name'].replace('\n', ''))
                st.write('ä»¤ç‰Œæ•°ç»Ÿè®¡')
                if os.path.exists(statistics):
                    df = pd.read_csv(statistics)
                    price = df.iloc[len(df) - 1, 1:].sum() * 0.00004
                    st.dataframe(df.rename(columns={'Unnamed: 0': 'ç±»åˆ«'}), use_container_width=True, hide_index=True)
                    st.write(f'æ€»è´¹ç”¨ï¼š**:blue[{price:.5f}]** å…ƒ')
                else:
                    st.write("æš‚æ— æ•°æ®ã€‚")


if __name__ == "__main__":
    config()
    if 'page' not in st.session_state:
        st.session_state['page'] = 'main'
    if 'files' not in st.session_state:
        st.session_state['files'] = []
    num = limit()
    st.title("ğŸ“¤ä¸Šä¼ æ–‡ä»¶")
    if 'name' in st.session_state and st.session_state['name']:
        if st.session_state['page'] == 'main':
            uploader(num)
        else:
            if st.button('è¿”å›'):
                st.session_state['page'] = 'main'
                st.experimental_rerun()
            setup(num, st.session_state['page'])
    else:
        st.warning('ç™»å½•å·²å¤±æ•ˆï¼Œè¯·è¿”å›é¦–é¡µé‡æ–°ç™»å½•ã€‚', icon='ğŸ•’')